{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/i-robles/ElipseJar/blob/main/Amazon_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<table>\n",
        "<tr>\n",
        "    <td>\n",
        "        <img src=\"https://www.wordstream.com/wp-content/uploads/2021/07/how-to-get-amazon-reviews.png\" width=\"200\"/>\n",
        "    </td>\n",
        "    <td style=\"text-align: left; vertical-align: top;\">\n",
        "        <h1><strong>Amazon Reviews</strong><br></h1>\n",
        "        <h4>Engineering Large Scale Data Analytics Systems<br>\n",
        "        ENSF 612 - Fall 2023</h4>\n",
        "    </td>\n",
        "</tr>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "PPqcEas0zbok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*** Note: run all the code the first time. For subsecuent runs, you can set the individual process flags below to False. This will avoid resetting spark, reloading the datasets or repeat computing intensive tasks that were previously computed and stored.\n"
      ],
      "metadata": {
        "id": "Vj1DYh7gVroz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "set_spark = True\n",
        "load_datasets = True\n",
        "inspect_data = True\n",
        "pre_process = True"
      ],
      "metadata": {
        "id": "Dtm4ra-vWu6K"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting Up Spark, Spark NLP and Required Modules**"
      ],
      "metadata": {
        "id": "f9WO6Sbd1kLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the capture magic command captures the output of the block to avoid clutter\n",
        "%%capture\n",
        "\n",
        "if set_spark:\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "  !wget https://dlcdn.apache.org/spark/spark-3.3.3/spark-3.3.3-bin-hadoop3.tgz\n",
        "  !tar -xvf spark-3.3.3-bin-hadoop3.tgz\n",
        "  !pip install findspark\n",
        "  !pip install -q spark-nlp\n",
        "  !pip install contractions\n",
        "\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.3-bin-hadoop3\"\n",
        "\n",
        "  import findspark\n",
        "  findspark.init()\n",
        "  findspark.find()\n",
        "  from pyspark.sql import SparkSession\n",
        "  import sparknlp\n",
        "\n",
        "  # Setting up 4 threads, potentially allowing a 4-core processor execute 4 tasks in parallel\n",
        "  # And adding the Spark NLP package to the Spark session\n",
        "  spark = SparkSession.builder\\\n",
        "      .appName(\"Colab\")\\\n",
        "      .master(\"local[4]\")\\\n",
        "      .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.1.4\")\\\n",
        "      .getOrCreate()\n",
        "\n",
        "  sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "Pxq1F4TUz2w8"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cloning Github Repository and Loading Datasets**"
      ],
      "metadata": {
        "id": "MsH2kePPYp5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if load_datasets:\n",
        "  !git clone https://github.com/MENG2023-TP/ENSF612-Project.git"
      ],
      "metadata": {
        "id": "GmhKWpfEiyK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31249571-7db1-4bce-819b-112237c11083"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ENSF612-Project' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ENSF612-Project/datasets"
      ],
      "metadata": {
        "id": "N2zZsE4mWIEa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e7fda10-4441-4613-f4f1-72ec3b815bbe"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All_Beauty_5.json  Cell_Phones_and_Accessories_5_subsample.json  Software_5.json\n",
            "Appliances_5.json  Musical_Instruments_5_subsample.json\t\t Video_Games_5_subsample.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_directory = 'ENSF612-Project/datasets'\n",
        "\n",
        "# Gets the list of files in the dataset directory that end in \".json\"\n",
        "json_files = [file for file in os.listdir(dataset_directory) if file.endswith('.json')]\n",
        "\n",
        "# Creates a list of full file paths\n",
        "file_paths = [os.path.join(dataset_directory, file) for file in json_files]"
      ],
      "metadata": {
        "id": "1Ezm-o9Qc6Ii"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Function to parse NDJSON (new line-delimited JSON) files and extract specific fields\n",
        "def parse_ndjson(line):\n",
        "    try:\n",
        "        # Parse the JSON line and return only reviewText asin and reviewerID\n",
        "        json_line = json.loads(line)\n",
        "        return (\n",
        "            json_line.get('overall', ''),\n",
        "            json_line.get('reviewText', '')\n",
        "        )\n",
        "    except json.JSONDecodeError:\n",
        "        # In case of error, skip this record and return None\n",
        "        return None"
      ],
      "metadata": {
        "id": "Xg7bWZmDeCWJ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if load_datasets:\n",
        "  # Initialize an empty RDD\n",
        "  X_rdd = spark.sparkContext.emptyRDD()\n",
        "  y_rdd = spark.sparkContext.emptyRDD()\n",
        "\n",
        "  # Read each file into an RDD, parse its ndjson objects if not None, and union with the existing RDD\n",
        "  for file_path in file_paths:\n",
        "      file_rdd = sc.textFile(file_path, 4) # Reads one of the files using its file_path\n",
        "      parsed_rdd = file_rdd.map(parse_ndjson).filter(lambda x: x is not None) # For each line in the file calls parse_ndjson, it also filters out None records\n",
        "      X_rdd = X_rdd.union(parsed_rdd.map(lambda x: (x[1],)))  # Extract review text (X)\n",
        "      y_rdd = y_rdd.union(parsed_rdd.map(lambda x: (x[0],)))  # Extract scores (y)\n",
        "\n",
        "  # convert the data_rdd to a distributed Spark DataFrame\n",
        "X_df = spark.createDataFrame(X_rdd, schema=['review']).cache()\n",
        "y_df = spark.createDataFrame(y_rdd, schema=['score']).cache()"
      ],
      "metadata": {
        "id": "sZauPMutEHHW"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Inspection**"
      ],
      "metadata": {
        "id": "leZYCxGgri_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_df # Shows the schema of the X_df dataframe, column name(s) and type(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1qYQz9JsRcf",
        "outputId": "d244649f-9211-4c83-9049-9f24d7c04820"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[review: string]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_df # Shows the schema of the y_df dataframe, column name(s) and type(s)"
      ],
      "metadata": {
        "id": "DAJd5ZPvge4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d7914e2-8896-4009-ba94-80153f20f6b4"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[score: double]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inspect_data:\n",
        "  X_count = X_df.count()\n",
        "  y_count = y_df.count()\n",
        "print(f\"Records in X_df: {X_count}\")\n",
        "print(f\"Records in y_df: {y_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXIqIZ070agO",
        "outputId": "cf6a8010-06de-4b69-c901-4b74745f8ad8"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Records in X_df: 118242\n",
            "Records in y_df: 118242\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_df.take(1) # Preview a single record"
      ],
      "metadata": {
        "id": "1JB8WoYvVfgr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62ea1a55-a4b1-4f41-8a46-80a0bdbc62ff"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(review='I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.')]"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note about checking null values: since parse_ndjson returns None for records it could not return both their score and text and later the None records are filtered out. X_rdd and y_rdd effectively do not contain any null values. The following code demonstrates that:"
      ],
      "metadata": {
        "id": "zWexURzKsOQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if inspect_data:\n",
        "  from pyspark.sql.functions import col, sum\n",
        "  null_X_count = X_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in X_df.columns]).cache()  # Count null values\n",
        "  null_y_count = y_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in y_df.columns]).cache()  # Count null values"
      ],
      "metadata": {
        "id": "RR6lncBXVMo9"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "null_X_count.show()\n",
        "null_y_count.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJEF_T7Dq2qc",
        "outputId": "9d4dcced-6e92-4596-dda8-1996cbaaf5bb"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+\n",
            "|review|\n",
            "+------+\n",
            "|     0|\n",
            "+------+\n",
            "\n",
            "+-----+\n",
            "|score|\n",
            "+-----+\n",
            "|    0|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Pre-Processing**"
      ],
      "metadata": {
        "id": "c_1op0sWyQwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expanding contractions\n",
        "\n",
        "Although this step in not estrictly neccesary. Expanding contractions can make the text clearer and more consistent for the model, which can improve its ability to interpret and analyze the words."
      ],
      "metadata": {
        "id": "XDaErNxGPY0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if pre_process:\n",
        "  from pyspark.sql.functions import udf\n",
        "  from pyspark.sql.types import StringType\n",
        "  import contractions\n",
        "\n",
        "  # Define the UDF for expanding contractions\n",
        "  def expand_contractions_text(text):\n",
        "      return contractions.fix(text)\n",
        "\n",
        "  expand_contractions_udf = udf(expand_contractions_text, StringType())\n",
        "\n",
        "  # Apply the UDF to the DataFrame to create a new column with expanded contractions\n",
        "  expanded_X_df = X_df.withColumn(\"expanded_review\", expand_contractions_udf(\"review\"))"
      ],
      "metadata": {
        "id": "k4-9n1ILO_YA"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining DocumentAssembler and Spark NLP components\n",
        "\n",
        "The DocumentAssembler is the initial step in a Spark NLP pipeline. It converts raw text into a structured Annotation format that subsequent Spark NLP annotators can utilize for processing."
      ],
      "metadata": {
        "id": "Ng2X3BH7L_Gr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
        "\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"expanded_review\") \\\n",
        "    .setOutputCol(\"document\")"
      ],
      "metadata": {
        "id": "xeNjrdmjLzDm"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Tokenization"
      ],
      "metadata": {
        "id": "ASy-qlIWyfeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")"
      ],
      "metadata": {
        "id": "j1KTfat5MLHe"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Text Cleaning"
      ],
      "metadata": {
        "id": "cfFm5Gm3zPLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\") \\\n",
        "    .setLowercase(True) \\\n",
        "    .setCleanupPatterns([\"[^A-Za-z'\\\\s]\"])  # remove punctuations and numbers"
      ],
      "metadata": {
        "id": "gJjSrwUCMPLE"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Stopword Removal"
      ],
      "metadata": {
        "id": "3skTysCEz_RV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_cleaner = StopWordsCleaner() \\\n",
        "    .setInputCols([\"normalized\"]) \\\n",
        "    .setOutputCol(\"cleanTokens\")"
      ],
      "metadata": {
        "id": "bjNBhlUVMS8t"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Stemming/Lemmatization.\n",
        "\n",
        "Stemming and lemmatization are both text normalization techniques that reduce words to their base or root form. Applying both can at times be redundant. For this application we decide to use Lemmatization.\n"
      ],
      "metadata": {
        "id": "A8p8I4tF0Ib1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the pretrained LemmatizerModel from Spark NLP\n",
        "lemmatizer = LemmatizerModel.pretrained() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"lemmatized\")"
      ],
      "metadata": {
        "id": "KnrQsjcEMdPY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5129c132-d4e2-4a08-cf36-f09589da879f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if pre_process:\n",
        "  # Define the Spark NLP pipeline\n",
        "  from pyspark.ml import Pipeline\n",
        "\n",
        "  preprocessing = Pipeline(stages=[\n",
        "      document_assembler,\n",
        "      tokenizer,\n",
        "      normalizer,\n",
        "      stop_words_cleaner,\n",
        "      lemmatizer\n",
        "  ])\n",
        "\n",
        "  processed = preprocessing.fit(expanded_X_df).transform(expanded_X_df).cache()"
      ],
      "metadata": {
        "id": "Z31hZhtxEOO8"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the processed data\n",
        "processed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7IiBlvAwu5T",
        "outputId": "0ed3e04a-4c97-4a72-c84a-75b4d7e1672c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|              review|     expanded_review|            document|               token|          normalized|         cleanTokens|          lemmatized|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|I like this as a ...|I like this as a ...|[{document, 0, 15...|[{token, 0, 0, I,...|[{token, 0, 0, i,...|[{token, 2, 5, li...|[{token, 2, 5, li...|\n",
            "|           good item|           good item|[{document, 0, 8,...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|\n",
            "|Fit my new LG dry...|Fit my new LG dry...|[{document, 0, 29...|[{token, 0, 2, Fi...|[{token, 0, 2, fi...|[{token, 0, 2, fi...|[{token, 0, 2, fi...|\n",
            "|Good value for el...|Good value for el...|[{document, 0, 29...|[{token, 0, 3, Go...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|\n",
            "|Price and deliver...|Price and deliver...|[{document, 0, 32...|[{token, 0, 4, Pr...|[{token, 0, 4, pr...|[{token, 0, 4, pr...|[{token, 0, 4, pr...|\n",
            "|I purchasaed a ne...|I purchasaed a ne...|[{document, 0, 17...|[{token, 0, 0, I,...|[{token, 0, 0, i,...|[{token, 2, 11, p...|[{token, 2, 11, p...|\n",
            "|         Good value.|         Good value.|[{document, 0, 10...|[{token, 0, 3, Go...|[{token, 0, 3, go...|[{token, 0, 3, go...|[{token, 0, 3, go...|\n",
            "|works great. we l...|works great. we l...|[{document, 0, 10...|[{token, 0, 4, wo...|[{token, 0, 4, wo...|[{token, 0, 4, wo...|[{token, 0, 4, wo...|\n",
            "|Luved it for the ...|Luved it for the ...|[{document, 0, 41...|[{token, 0, 4, Lu...|[{token, 0, 4, lu...|[{token, 0, 4, lu...|[{token, 0, 4, lu...|\n",
            "|Be careful, NewAi...|Be careful, NewAi...|[{document, 0, 39...|[{token, 0, 1, Be...|[{token, 0, 1, be...|[{token, 3, 9, ca...|[{token, 3, 9, ca...|\n",
            "|We would give les...|We would give les...|[{document, 0, 58...|[{token, 0, 1, We...|[{token, 0, 1, we...|[{token, 9, 12, g...|[{token, 9, 12, g...|\n",
            "|Be careful, NewAi...|Be careful, NewAi...|[{document, 0, 39...|[{token, 0, 1, Be...|[{token, 0, 1, be...|[{token, 3, 9, ca...|[{token, 3, 9, ca...|\n",
            "|We would give les...|We would give les...|[{document, 0, 58...|[{token, 0, 1, We...|[{token, 0, 1, we...|[{token, 9, 12, g...|[{token, 9, 12, g...|\n",
            "|We would give les...|We would give les...|[{document, 0, 58...|[{token, 0, 1, We...|[{token, 0, 1, we...|[{token, 9, 12, g...|[{token, 9, 12, g...|\n",
            "|       Great product|       Great product|[{document, 0, 12...|[{token, 0, 4, Gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|\n",
            "|Did the job for f...|Did the job for f...|[{document, 0, 38...|[{token, 0, 2, Di...|[{token, 0, 2, di...|[{token, 8, 10, j...|[{token, 8, 10, j...|\n",
            "|               cheap|               cheap|[{document, 0, 4,...|[{token, 0, 4, ch...|[{token, 0, 4, ch...|[{token, 0, 4, ch...|[{token, 0, 4, ch...|\n",
            "|Great deal and fa...|Great deal and fa...|[{document, 0, 28...|[{token, 0, 4, Gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|[{token, 0, 4, gr...|\n",
            "|All was fine, pro...|All was fine, pro...|[{document, 0, 39...|[{token, 0, 2, Al...|[{token, 0, 2, al...|[{token, 8, 11, f...|[{token, 8, 11, f...|\n",
            "|         received ok|         received ok|[{document, 0, 10...|[{token, 0, 7, re...|[{token, 0, 7, re...|[{token, 0, 7, re...|[{token, 0, 7, re...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the first row of the DataFrame\n",
        "first_row = processed.first()\n",
        "\n",
        "# Print first row with its content\n",
        "print(\"Review:\", first_row['review'])\n",
        "print(\"Document:\", [doc.result for doc in first_row['document']])\n",
        "print(\"Token:\", [tok.result for tok in first_row['token']])\n",
        "print(\"Normalized:\", [norm.result for norm in first_row['normalized']])\n",
        "print(\"Clean Tokens:\", [clean.result for clean in first_row['cleanTokens']])\n",
        "print(\"Lemmatized:\", [lemma.result for lemma in first_row['lemmatized']])"
      ],
      "metadata": {
        "id": "1PW-gHXqJ0t5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628f61fb-005a-4e4a-8ac5-c75f92fd8988"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.\n",
            "Document: ['I like this as a vent as well as something that will keep house warmer in winter.  I sanded it and then painted it the same color as the house.  Looks great.']\n",
            "Token: ['I', 'like', 'this', 'as', 'a', 'vent', 'as', 'well', 'as', 'something', 'that', 'will', 'keep', 'house', 'warmer', 'in', 'winter', '.', 'I', 'sanded', 'it', 'and', 'then', 'painted', 'it', 'the', 'same', 'color', 'as', 'the', 'house', '.', 'Looks', 'great', '.']\n",
            "Normalized: ['i', 'like', 'this', 'as', 'a', 'vent', 'as', 'well', 'as', 'something', 'that', 'will', 'keep', 'house', 'warmer', 'in', 'winter', 'i', 'sanded', 'it', 'and', 'then', 'painted', 'it', 'the', 'same', 'color', 'as', 'the', 'house', 'looks', 'great']\n",
            "Clean Tokens: ['like', 'vent', 'well', 'something', 'keep', 'house', 'warmer', 'winter', 'sanded', 'painted', 'color', 'house', 'looks', 'great']\n",
            "Lemmatized: ['like', 'vent', 'well', 'something', 'keep', 'house', 'warm', 'winter', 'sand', 'paint', 'color', 'house', 'look', 'great']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature engineering**"
      ],
      "metadata": {
        "id": "yoZ0k4Olwv79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF Vectorization"
      ],
      "metadata": {
        "id": "wHIgXtpx10fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import HashingTF, CountVectorizer, IDF\n",
        "\n",
        "#hashingTF = HashingTF(numFeatures=20) \\\n",
        "#    .setInputCol(\"lemmatized\") \\\n",
        "#    .setOutputCol(\"rawFeatures\")\n",
        "\n",
        "countVectorizer = CountVectorizer(vocabSize = 500) \\\n",
        "    .setInputCol(\"lemmatized\") \\\n",
        "    .setOutputCol(\"rawFeatures\")\n",
        "\n",
        "tfidf_vectorizer = IDF() \\\n",
        "    .setInputCol(\"rawFeatures\") \\\n",
        "    .setOutputCol(\"tfidf_features\")"
      ],
      "metadata": {
        "id": "6l3qrOMt11nS"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing Machine Learning Models**"
      ],
      "metadata": {
        "id": "vuGdTEVu41d8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Model Selection"
      ],
      "metadata": {
        "id": "T9k619pt55OG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC\n",
        "\n",
        "# Logistic Regression Pipeline\n",
        "lr = LogisticRegression(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
        "lr_pipeline = Pipeline(stages=[preprocessing, countVectorizer, tfidf_vectorizer, lr])\n",
        "\n",
        "# Naive Bayes Pipeline\n",
        "nb = NaiveBayes(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
        "nb_pipeline = Pipeline(stages=[preprocessing, countVectorizer, tfidf_vectorizer, nb])\n",
        "\n",
        "# Linear SVC Pipeline\n",
        "svc = LinearSVC(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
        "svc_pipeline = Pipeline(stages=[preprocessing, countVectorizer, tfidf_vectorizer, svc])\n"
      ],
      "metadata": {
        "id": "juOHzf1S40Rq"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating Models**"
      ],
      "metadata": {
        "id": "9C02hm6Z5IWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QNcN992T4z9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next steps:\n",
        "\n",
        "Feature extraction (Bag of Words, TF-IDF, word embeddings Word2Vec)\n",
        "\n",
        "Vectorization (Count Vectorizer, TfidfVectorizer)\n",
        "\n",
        "Model selection (LogisticRegression, Nayve Bayes, SVM or unsupervised learning)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Additional steps (optional to improve accuracy):\n",
        "\n",
        "Speech tagging (before stop word removal)\n",
        "\n",
        "N-grams to use along with TD-IDF"
      ],
      "metadata": {
        "id": "GgxZ4b794R6g"
      }
    }
  ]
}